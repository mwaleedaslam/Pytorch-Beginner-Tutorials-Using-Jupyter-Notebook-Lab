{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06dd6260-2f08-4e80-b4ab-98af36644747",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "### - Already trained model that can be trained and used for other classes\n",
    "### - Only require to update last layer of the already trained model\n",
    "# Further 3 things to learn\n",
    "### 1- Folder Catergorization\n",
    "### 2- Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1a37630-2a91-4e2a-b6a6-bf21c2107aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33765746-2ea2-422e-acfb-83ad1484ed53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e019d18f-2750-42cb-8887-eba0a71714ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arrays used for normalizing data\n",
    "mean = np.array([0.5, 0.5, 0.5])\n",
    "std = np.array([0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a811a4e-fdd1-4928-a04f-477f4ff6bfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db9aed64-b2b7-4c46-adc4-5172c1f44991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "dir_path = 'data/hymenoptera_data'\n",
    "# provide the paths of image dataset and load image data\n",
    "# data transformation is also done\n",
    "img_data = {x: datasets.ImageFolder(os.path.join(dir_path, x), data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0579d669-53df-497d-83ed-bb68b3765c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "dataloaders = {x: torch.utils.data.DataLoader(img_data[x], batch_size=4, shuffle=True, num_workers=0)\n",
    "               for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa3ddbb2-9783-401e-9233-6c167b67cbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "# get the classes\n",
    "dataset_sizes = {x: len(img_data[x]) for x in ['train', 'val']}\n",
    "class_names = img_data['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03ea1190-8f56-415c-aa63-fb910f266223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(model, criterion, optimizer, scheduler, epochs=20):\n",
    "    time_start = time.time()\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch}/{epochs-1}')\n",
    "        print('-'*10)\n",
    "        \n",
    "        # training and validation stage of each epoch\n",
    "        for state in ['train', 'val']:\n",
    "            if state == 'train':\n",
    "                model.train() # training model\n",
    "            else:\n",
    "                model.eval() # evaluating model\n",
    "                \n",
    "            model_running_loss = 0.0\n",
    "            model_running_corrects = 0\n",
    "            \n",
    "            # iteration in dataset\n",
    "            for images, labels in dataloaders[state]:\n",
    "                images.to(device)\n",
    "                labels.to(device)\n",
    "                \n",
    "                # forward pass and tracking history of training state\n",
    "                with torch.set_grad_enabled(state == 'train'):\n",
    "                    out = model(images)\n",
    "                    # value, index\n",
    "                    _, predictions = torch.max(out, 1)\n",
    "                    loss = criterion(out, labels)\n",
    "                    \n",
    "                    # backward pass and optimizer history of training state\n",
    "                    if state == 'train':\n",
    "                        # backward pass\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # results of training\n",
    "                model_running_loss += loss.item() * images.size(0)\n",
    "                model_running_corrects += torch.sum(predictions == labels.data)\n",
    "                \n",
    "            if state == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = model_running_loss / dataset_sizes[state]\n",
    "            epoch_accuracy = model_running_corrects.double() / dataset_sizes[state]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(state, epoch_loss, epoch_accuracy))\n",
    "\n",
    "            # copy the best model\n",
    "            if state == 'val' and epoch_accuracy > best_accuracy:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "    print()\n",
    "    time_end = time.time() - time_start\n",
    "    print('Training completion time: {:.0f}m {:.0f}s'.format(time_end // 60, time_end % 60))\n",
    "    print('The best val accuracy: {:4f}'.format(best_accuracy))\n",
    "\n",
    "    # load the best model with weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb12592-cd25-460d-9692-978960ae8a86",
   "metadata": {},
   "source": [
    "# Method 1: To train transfer models\n",
    "## Finetuning of Convolutionel Network\n",
    "### - Load the pretrained model\n",
    "### - Reset the last fully connected layer of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "203b3c8f-fcc4-445f-a171-880caae4ad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# get number of input features from the last layer\n",
    "num_feat = model.fc.in_features\n",
    "\n",
    "# create a new fully connected layer and assign it to the last layer\n",
    "model.fc = nn.Linear(num_feat, 2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d2ae06f-32ae-4ed8-8bb8-f1a4146d5e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion =  nn.CrossEntropyLoss() # nn.CrossEntropyLoss() automaticsally applies Softmax\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2938909b-837d-4f09-94b4-1b01098b2546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n",
      "train Loss: 0.6287 Acc: 0.6680\n",
      "val Loss: 0.4431 Acc: 0.8562\n",
      "Epoch 1/3\n",
      "----------\n",
      "train Loss: 0.5073 Acc: 0.7541\n",
      "val Loss: 0.3419 Acc: 0.8824\n",
      "Epoch 2/3\n",
      "----------\n",
      "train Loss: 0.4777 Acc: 0.7951\n",
      "val Loss: 0.2899 Acc: 0.8954\n",
      "Epoch 3/3\n",
      "----------\n",
      "train Loss: 0.4083 Acc: 0.8402\n",
      "val Loss: 0.2726 Acc: 0.8889\n",
      "\n",
      "Training completion time: 1m 35s\n",
      "The best val accuracy: 0.895425\n"
     ]
    }
   ],
   "source": [
    "# use scheduler for updating learning rate\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# every 7 epoch, the lr is multiplied by gamma (means lr update only 10% at each 7 epochs)\n",
    "lr_sched = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = training_model(model, criterion, optimizer, lr_sched, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de3cad-cdf4-47f2-8900-ddb0326825f3",
   "metadata": {},
   "source": [
    "# Method 2: To train transfer models\n",
    "## Fix feature extractor\n",
    "### Freeze all the network except the last fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53c33e54-5efb-40f2-b432-c3c6ef34bdcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set requires_grad = False to freeze the parameters so that the gradients are not computed in backward()\n",
    "model_conv_net = torchvision.models.resnet18(pretrained=True)\n",
    "for parameters in model_conv_net.parameters():\n",
    "    parameters.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_features = model_conv_net.fc.in_features\n",
    "model_conv_net.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "model_conv_net = model_conv_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2deab248-a03d-4a7b-9e69-559166fab937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# only parameters of last layer are optimized as opposed to method 1.\n",
    "optimizer_conv_net = optim.SGD(model_conv_net.fc.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22f1d864-d466-4de7-aee8-dc3ff7811961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n",
      "train Loss: 0.5800 Acc: 0.6639\n",
      "val Loss: 0.3948 Acc: 0.8105\n",
      "Epoch 1/3\n",
      "----------\n",
      "train Loss: 0.3767 Acc: 0.8279\n",
      "val Loss: 0.1602 Acc: 0.9477\n",
      "Epoch 2/3\n",
      "----------\n",
      "train Loss: 0.3639 Acc: 0.8279\n",
      "val Loss: 0.1705 Acc: 0.9412\n",
      "Epoch 3/3\n",
      "----------\n",
      "train Loss: 0.5237 Acc: 0.7869\n",
      "val Loss: 0.2278 Acc: 0.9216\n",
      "\n",
      "Training completion time: 0m 46s\n",
      "The best val accuracy: 0.947712\n"
     ]
    }
   ],
   "source": [
    "# use scheduler for updating learning rate\n",
    "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# every 7 epoch, the lr is multiplied by gamma (means lr update only 10% at each 7 epochs)\n",
    "lr_schedu = lr_scheduler.StepLR(optimizer_conv_net, step_size=7, gamma=0.1)\n",
    "model_conv_net = training_model(model_conv_net, criterion, optimizer_conv_net, lr_schedu, epochs=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
