{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be53d6bb",
   "metadata": {},
   "source": [
    "# Use Builtin RNN, GRU, and LSTM\n",
    "## Major Used Concepts\n",
    "### 1. MNIST Dataset for Image Classification\n",
    "### 2. Treat the Image as Sequence Instead of Squeezing it into 1D i.e. (one dimenstion of the image will be considered as a sequence, the other dimention will be considered as a feature size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d26dc37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38bd8c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ee7ed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set important parameters\n",
    "input_size = 28 # because 1 row = 28\n",
    "sequence_size = 28 # because 1 col = 28\n",
    "n_layers = 2\n",
    "hidden_size = 128 # can be tried different sizes\n",
    "n_classes = 10 # because MNIST data will have total 10 classes\n",
    "l_rate = 0.001\n",
    "batch_size = 100\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e0427e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor() Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "# MNIST data & transformation of data into tensor\n",
    "train_data = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "print(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "315a8bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load batchsize of 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4890df30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for i in test_loader:\n",
    "    print(i[0].shape,i[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1cd0232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0824, 0.7569, 0.1176, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.1608, 0.9529, 0.3216, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4000, 0.9922, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.5176, 0.9922, 0.1608, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4000, 0.9961, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.3216, 0.9922, 0.8314, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4000, 0.9922, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4000, 0.9961, 0.3569, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0431, 0.8353, 0.9961, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.7176, 0.8353, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.2000, 0.9882, 0.8353, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.4431, 0.9922, 0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.2000, 0.9922, 0.7961, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5961, 0.9882, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.2000, 0.9882, 0.6353, 0.7961, 0.1608, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.8392, 0.7529, 0.0000, 0.0000, 0.0000, 0.1608, 0.2000, 0.2000,\n",
      "          0.5176, 0.9922, 0.9961, 0.9922, 0.1608, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3216,\n",
      "          0.9922, 0.7529, 0.4000, 0.5569, 0.7961, 0.9529, 0.9922, 0.9882,\n",
      "          0.9922, 0.9882, 0.9137, 0.4353, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3216,\n",
      "          0.9961, 0.9922, 0.9961, 0.9922, 0.9961, 0.9137, 0.7961, 0.4784,\n",
      "          0.9176, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.4392, 0.5922, 0.5137, 0.1961, 0.1961, 0.1176, 0.0000, 0.0000,\n",
      "          0.7569, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5961, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.9137, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.9176, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.9137, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          1.0000, 0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3216,\n",
      "          0.9922, 0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4039,\n",
      "          1.0000, 0.9922, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784,\n",
      "          0.8353, 0.8314, 0.0784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# test and have look of 1 batch of data\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.shape, labels.shape)\n",
    "print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22ac4563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot some samples\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded851e-2e00-42bb-aed5-2c370104ea12",
   "metadata": {},
   "source": [
    "# 1. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4379f0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, n_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # batch_first indicates that the batch should be first dimention\n",
    "        # input_shape = (batch_size, sequence_size, input_size)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # 2 inputs are required in the befinning -> 1) initial hidden state, 2) input\n",
    "        # inputs.size(0) presents batch size\n",
    "        h0 = torch.zeros(self.n_layers, inputs.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # deliver 2 outputs -> 1) output of all the steps, 2) h_n for the step n\n",
    "        # we require output only\n",
    "        # output = (batch_size, sequence_size, hidden_size) = (N, 28, 128)\n",
    "        output, _ = self.rnn(inputs, h0)\n",
    "        \n",
    "        # we require (batch_size, hidden_size) = (N, 128)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c56a8a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, n_layers, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2f0f607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion =  nn.CrossEntropyLoss() # nn.CrossEntropyLoss() automaticsally applies Softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6fde11b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss = 1.0533\n",
      "epoch 1 / 2, step 200 / 600, loss = 0.7983\n",
      "epoch 1 / 2, step 300 / 600, loss = 0.5220\n",
      "epoch 1 / 2, step 400 / 600, loss = 0.5440\n",
      "epoch 1 / 2, step 500 / 600, loss = 0.3396\n",
      "epoch 1 / 2, step 600 / 600, loss = 0.5086\n",
      "epoch 2 / 2, step 100 / 600, loss = 0.5925\n",
      "epoch 2 / 2, step 200 / 600, loss = 0.4452\n",
      "epoch 2 / 2, step 300 / 600, loss = 0.1920\n",
      "epoch 2 / 2, step 400 / 600, loss = 0.4130\n",
      "epoch 2 / 2, step 500 / 600, loss = 0.4564\n",
      "epoch 2 / 2, step 600 / 600, loss = 0.3406\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "n_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape -> 100, 1, 28, 28\n",
    "        # required shape -> 100, 28, 28\n",
    "        images = images.reshape(-1, sequence_size, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {n_epochs}, step {i+1} / {n_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7182edbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 91.99\n"
     ]
    }
   ],
   "source": [
    "# testing model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # origin shape -> 100, 1, 28, 28\n",
    "        # required shape -> 100, 28, 28\n",
    "        images = images.reshape(-1, sequence_size, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predictions = torch.max(out, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f82eec-906e-45b5-8ddd-adfa8a3f08d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. GRU Model\n",
    "### Code is same as RNN\n",
    "### Inputs paramters are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "688800c8-e592-45a5-94ea-d400f856b658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, n_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # batch_first indicates that the batch should be first dimention\n",
    "        # input_shape = (batch_size, sequence_size, input_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # 2 inputs are required in the befinning -> 1) initial hidden state, 2) input\n",
    "        # inputs.size(0) presents batch size\n",
    "        h0 = torch.zeros(self.n_layers, inputs.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # deliver 2 outputs -> 1) output of all the steps, 2) h_n for the step n\n",
    "        # we require output only\n",
    "        # output = (batch_size, sequence_size, hidden_size) = (N, 28, 128)\n",
    "        output, _ = self.gru(inputs, h0)\n",
    "        \n",
    "        # we require (batch_size, hidden_size) = (N, 128)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9783e99-4f67-4a17-a0ab-bbc2f1317fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GRU(input_size, hidden_size, n_layers, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a2434e4-3309-4037-8a25-9e339f721fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion =  nn.CrossEntropyLoss() # nn.CrossEntropyLoss() automaticsally applies Softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c78efdb-3846-4301-8c54-ef4082174fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss = 0.6384\n",
      "epoch 1 / 2, step 200 / 600, loss = 0.3792\n",
      "epoch 1 / 2, step 300 / 600, loss = 0.2611\n",
      "epoch 1 / 2, step 400 / 600, loss = 0.1157\n",
      "epoch 1 / 2, step 500 / 600, loss = 0.1610\n",
      "epoch 1 / 2, step 600 / 600, loss = 0.1392\n",
      "epoch 2 / 2, step 100 / 600, loss = 0.0897\n",
      "epoch 2 / 2, step 200 / 600, loss = 0.0600\n",
      "epoch 2 / 2, step 300 / 600, loss = 0.1132\n",
      "epoch 2 / 2, step 400 / 600, loss = 0.2805\n",
      "epoch 2 / 2, step 500 / 600, loss = 0.1234\n",
      "epoch 2 / 2, step 600 / 600, loss = 0.1496\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "n_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape -> 100, 1, 28, 28\n",
    "        # required shape -> 100, 28, 28\n",
    "        images = images.reshape(-1, sequence_size, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {n_epochs}, step {i+1} / {n_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c9bf14a-6abc-483e-88dd-6d0ba86431b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 97.29\n"
     ]
    }
   ],
   "source": [
    "# testing model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # origin shape -> 100, 1, 28, 28\n",
    "        # required shape -> 100, 28, 28\n",
    "        images = images.reshape(-1, sequence_size, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predictions = torch.max(out, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9cde7-9139-4dbb-b7c0-4bffbea6324b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. LSTM Model\n",
    "### Code is same as RNN or GRU\n",
    "### The only difference: Require an initial Cell state (c0)\n",
    "### Input paramters are still same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e4c3261-fcfc-4f58-a0df-201050375473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, n_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # batch_first indicates that the batch should be first dimention\n",
    "        # input_shape = (batch_size, sequence_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # 2 inputs are required in the befinning -> 1) initial hidden state, 2) input\n",
    "        # inputs.size(0) presents batch size\n",
    "        h0 = torch.zeros(self.n_layers, inputs.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # initial tensor for initial Cell state\n",
    "        c0 = torch.zeros(self.n_layers, inputs.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # deliver 2 outputs -> 1) output of all the steps, 2) h_n for the step n\n",
    "        # we require output only\n",
    "        # output = (batch_size, sequence_size, hidden_size) = (N, 28, 128)\n",
    "        output, _ = self.lstm(inputs, (h0, c0))\n",
    "        \n",
    "        # we require (batch_size, hidden_size) = (N, 128)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92361cf1-3e86-48dc-a923-18bd21917e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, n_layers, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0eb01bf0-5c48-4cfb-9c1f-0f85eb0e9184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion =  nn.CrossEntropyLoss() # nn.CrossEntropyLoss() automaticsally applies Softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "097cb6dc-9165-4d8f-99bf-b6403a28e68e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss = 0.6825\n",
      "epoch 1 / 2, step 200 / 600, loss = 0.4827\n",
      "epoch 1 / 2, step 300 / 600, loss = 0.2832\n",
      "epoch 1 / 2, step 400 / 600, loss = 0.2277\n",
      "epoch 1 / 2, step 500 / 600, loss = 0.2124\n",
      "epoch 1 / 2, step 600 / 600, loss = 0.1907\n",
      "epoch 2 / 2, step 100 / 600, loss = 0.1045\n",
      "epoch 2 / 2, step 200 / 600, loss = 0.1406\n",
      "epoch 2 / 2, step 300 / 600, loss = 0.1437\n",
      "epoch 2 / 2, step 400 / 600, loss = 0.1081\n",
      "epoch 2 / 2, step 500 / 600, loss = 0.1270\n",
      "epoch 2 / 2, step 600 / 600, loss = 0.0599\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "n_steps = len(train_loader)\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape -> 100, 1, 28, 28\n",
    "        # required shape -> 100, 28, 28\n",
    "        images = images.reshape(-1, sequence_size, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {n_epochs}, step {i+1} / {n_steps}, loss = {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "982d0577-4484-4157-b04f-218d940fb5c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 97.04\n"
     ]
    }
   ],
   "source": [
    "# testing model\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # origin shape -> 100, 1, 28, 28\n",
    "        # required shape -> 100, 28, 28\n",
    "        images = images.reshape(-1, sequence_size, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        out = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predictions = torch.max(out, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96444583-ecf2-4396-b33e-d736eb7fbe39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
